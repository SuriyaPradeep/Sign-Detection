{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73527a48",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36cee050",
   "metadata": {},
   "source": [
    "Speech impaired people use hand signs and gestures to communicate. Normal people face difficulty in understanding their language. Hence there is a need of a system which recognizes the different signs, gestures and conveys the information to the normal people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a42b68",
   "metadata": {},
   "source": [
    "### Import and add dependncy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724deb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os#Work with filepaths\n",
    "import cv2#Open Cv\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e669b1",
   "metadata": {},
   "source": [
    "### Taking keypoints using mediapipe holistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up mediapipe holistic's\n",
    "#Media pipe holistic to make detection\n",
    "#Media pipe drawing will draw those points\n",
    "#We will create them as function to easily access them\n",
    "mp_holistic=mp.solutions.holistic#Holistic model\n",
    "mp_drawing=mp.solutions.drawing_utils#Drawing the utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa79185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating mediapipe detection function\n",
    "def mediapipe_detection(image,model):\n",
    "    #To the function we pass image and holistic model for detection\n",
    "    #So when wwe get feed from opencv it is of format bgr(blue,green,red)\n",
    "    #For detection we need them to be rgb we will change that using opencv\n",
    "    #Here image is the frames from opencv\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)#Color conversion\n",
    "    image.flags.writeable=False#Image is no longer writeable\n",
    "    results=model.process(image)#Making detection\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)#Color conversion\n",
    "    image.flags.writeable=True#Image is writeable again\n",
    "    return image,results #Returning the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481558af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To draw the points onto the image\n",
    "def draw_landmarks(image,results):\n",
    "    #drawing landmarks using mp_drawings.draw_landmark\n",
    "    #For Face\n",
    "    mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_TESSELATION)\n",
    "    #For Pose\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS)\n",
    "    #For Left Hand\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
    "    #For Right Hand\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
    "    #Pose landmark shows what landmark connected to other landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ace091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mp_holistic.FACEMESH_TESSELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa79edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatted draw style landmarks\n",
    "def draw_styled_landmarks(image,result):\n",
    "    #For Face\n",
    "    mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_TESSELATION\n",
    "                              #color landmark\n",
    "                             ,mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1)\n",
    "                             #color connections\n",
    "                             ,mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1))\n",
    "    #For Pose\n",
    "    mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS\n",
    "                             #color landmark\n",
    "                             ,mp_drawing.DrawingSpec(color=(80,22,10),thickness=2,circle_radius=4)\n",
    "                             #color connections\n",
    "                             ,mp_drawing.DrawingSpec(color=(80,44,121),thickness=2,circle_radius=2))\n",
    "    #For Left Hand\n",
    "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS\n",
    "                             #color landmark\n",
    "                             ,mp_drawing.DrawingSpec(color=(121,22,76),thickness=2,circle_radius=4)\n",
    "                             #color connections\n",
    "                             ,mp_drawing.DrawingSpec(color=(121,44,250),thickness=2,circle_radius=2))\n",
    "    #For Right Hand\n",
    "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS\n",
    "                             #color landmark\n",
    "                             ,mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=4)\n",
    "                             #color connections\n",
    "                             ,mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing video through webcm using OpenCV\n",
    "#We loop thorugh all frames in camera to create video\n",
    "cap=cv2.VideoCapture(0)#To acccess our webcam \n",
    "#here 0 represents device\n",
    "\n",
    "#Accessing the holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():#It checks wheather we are accessing or not\n",
    "        #Read feed\n",
    "        ret,frame=cap.read()#It reads our frames\n",
    "        \n",
    "        #Make detections\n",
    "        image,results=mediapipe_detection(frame,holistic)\n",
    "        \n",
    "        #Drwing Landmarks\n",
    "        draw_styled_landmarks(image,results)\n",
    "        \n",
    "        #To showw to screen\n",
    "        #Rendering\n",
    "        cv2.imshow(\"OpenCv Feed\",image)\n",
    "\n",
    "        #To Exit o break the feed\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break #it waits and if we press q breaks the loop\n",
    "    cap.release()#It releases the webcam\n",
    "    cv2.destroyAllWindows()#Destroy the cv window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.face_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5033fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame,results)\n",
    "plt.imshow(frame)\n",
    "#This show last frame captured\n",
    "#Color Conversion\n",
    "plt.imshow(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d551288",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_styled_landmarks(frame,results)\n",
    "plt.imshow(frame)\n",
    "#This show last frame captured\n",
    "#Color Conversion\n",
    "plt.imshow(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2bf43a",
   "metadata": {},
   "source": [
    "### Extract Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d25ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.face_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining values using list comprenhension\n",
    "def extract_keypoints(results):\n",
    "    #Pose\n",
    "    # If else statement returns zero array if results is empty\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    #We need to flatten it to get in one array\n",
    "    #Left hand\n",
    "    lh=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "    #Right hand\n",
    "    rh=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    #Face\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    #returning the values in a single concatenated array\n",
    "    return np.concatenate([pose,face,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643f228",
   "metadata": {},
   "source": [
    "### Setup folders for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for exported data,Numpyarray\n",
    "DATA_PATH=os.path.join('MP_data')\n",
    "\n",
    "#Actions we are going to try to detect\n",
    "actions=np.array(['hello','thanks','loveyou'])\n",
    "#We use 30 different frames of data to detect actions\n",
    "#30 videos worth of data\n",
    "no_sequences=30\n",
    "#Videos are going to be length of 30 frames\n",
    "sequence_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating folders to save the data\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            #makedirs will create sub folders\n",
    "            os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "#We will create 30 folders for every actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a09ab",
   "metadata": {},
   "source": [
    "### Collecting data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda38c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing video through webcm using OpenCV\n",
    "#We loop thorugh all frames in camera to create video\n",
    "cap=cv2.VideoCapture(0)#To acccess our webcam \n",
    "#here 0 represents device\n",
    "\n",
    "#Accessing the holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    #Loop through actions:hello,thanks etc\n",
    "    for action in actions:\n",
    "        #Loop through videos each action 30 videos\n",
    "        for sequence in range(no_sequences):\n",
    "            #Loop through each frame per video 30 frames\n",
    "            for frame_num in range(sequence_length):\n",
    "                #Read feed\n",
    "                ret,frame=cap.read()#It reads our frames\n",
    "\n",
    "                #Make detections\n",
    "                image,results=mediapipe_detection(frame,holistic)\n",
    "\n",
    "                #Drwing Landmarks\n",
    "                draw_styled_landmarks(image,results)\n",
    "                    \n",
    "                #Applying wait logic \n",
    "                #To give break between videos and say to user what video he is at\n",
    "                if frame_num==0:\n",
    "                    cv2.putText(image,'STARTING COLLECION',(120,200),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),4,cv2.LINE_AA)\n",
    "                    cv2.putText(image,f'Collecting frames for {action} Video number {sequence}'\n",
    "                                    ,(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)#This waits for 2 second after every video captured\n",
    "                    \n",
    "                else:\n",
    "                    cv2.putText(image,f'Collecting frames for {action} Video number {sequence}'\n",
    "                                    ,(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "\n",
    "                #Extracting the keypoints\n",
    "                keypoints=extract_keypoints(results)\n",
    "                #Saving the extracted keypoints\n",
    "                #Path to save the key points\n",
    "                npy_path=os.path.join(DATA_PATH,action,str(sequence),str(frame_num))\n",
    "                #Saving keypoints\n",
    "                np.save(npy_path,keypoints)\n",
    "                    \n",
    "                #To show to screen\n",
    "                #Rendering\n",
    "                cv2.imshow(\"OpenCv Feed\",image)\n",
    "\n",
    "                #To Exit o break the feed\n",
    "                if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "                    break #it waits and if we press q breaks the loop\n",
    "    cap.release()#It releases the webcam\n",
    "    cv2.destroyAllWindows()#Destroy the cv window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To destroy open cv in middle\n",
    "cap.release()#It releases the webcam\n",
    "cv2.destroyAllWindows()#Destroy the cv window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50998808",
   "metadata": {},
   "source": [
    "### Preprocessing Data and Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To split data for training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "#To convert data into one encoded data\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcf886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating label map\n",
    "label_map={label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48725740",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map\n",
    "#We created dictionary for labels with set of id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In preprocessing we put every frame np file in single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating 2 blank arrays\n",
    "sequences,labels=[],[]\n",
    "#here sequences represent our feature data(x) and labels represent labels(y)\n",
    "#Going through our actions\n",
    "for action in actions:\n",
    "    #Going through our 30 videos\n",
    "    for sequence in range(no_sequences):\n",
    "        #Creating a blank array window\n",
    "        window=[]\n",
    "        #Going through each frames\n",
    "        for frame_num in range(sequence_length):\n",
    "            #Loading up the respective frame using np.load()\n",
    "            res=np.load(os.path.join(DATA_PATH,action,str(sequence),f\"{frame_num}.npy\"))\n",
    "            #Adding the value to the window array\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(sequences)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(labels).astype(int)\n",
    "y\n",
    "#Here \n",
    "#[1,0,0]-hello\n",
    "#[0,1,0]-thanks\n",
    "#[0,0,1]-bye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forming training and testing partition\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.04,random_state=1)\n",
    "#6 for test other 114 to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c518ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce380c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63785ded",
   "metadata": {},
   "source": [
    "### Building and Training LSTM Neural Network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d8d4720",
   "metadata": {},
   "source": [
    "Why using this type of neural network(mediapipe and LSTM)?\n",
    "-Requires less data to produce high accuracy model\n",
    "-Faster to train\n",
    "-Faster detection as it is more simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing sequential model,LSTM layer and dense layer\n",
    "from tensorflow.keras.models import Sequential #Allows us to build sequential neural model\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "#tensorboard --logdir=. use this to check logs\n",
    "#It allows us to logging inside tensorboard to trace and moniter our model as it is training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard allow us to moniter our accuracy as it is training\n",
    "#Create a log directory to setuo tensorboard callbacks\n",
    "log_dir=os.path.join('Logs')\n",
    "tb_callback=TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9351c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the model\n",
    "model=Sequential()\n",
    "#Adding 3 sets of LSTM models\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(30,1662)))\n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "#Next layer is dense layer which we dont need to return sequences\n",
    "#Take a look at andrew ng deeplearing specializations\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(actions.shape[0],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "#categorical_crossentropy is needed for multi classification model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a51e215c",
   "metadata": {},
   "source": [
    "An epoch is a complete iteration through the entire training dataset in one cycle for training the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and train the model\n",
    "model.fit(X_train,y_train,epochs=2000,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95551f5a",
   "metadata": {},
   "source": [
    "### Making Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    print(f\"\\nPrediction:{actions[np.argmax(res[0])]}\\tActual Value:{actions[np.argmax(y_test[0])]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adec19",
   "metadata": {},
   "source": [
    "### Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9373bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b7d32",
   "metadata": {},
   "source": [
    "### Evalution using confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f78c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff13af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the predicted classes\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a962165",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5343260",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d9497",
   "metadata": {},
   "source": [
    "### Testing in Realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To render the probality\n",
    "colors=[(245,117,16),(117,245,16),(16,117,245)]\n",
    "#One color for each actions\n",
    "def prob_viz(res,actions,input_frame,colors):\n",
    "    output_frame=input_frame.copy()#Copy of the frames\n",
    "    for num,prob in enumerate(res):\n",
    "        #Dynamicllay placing the rectangle\n",
    "        cv2.rectangle(output_frame,(0,60+num*40),(int(prob*100),90+num*40),colors[num],-1)\n",
    "        cv2.putText(output_frame,actions[num],(0,85+num*40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78168ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prob_viz(res,actions,image,colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New detection variables\n",
    "sequence=[]#Collect 30 frames for prediction\n",
    "sentence=[]#Concatenate our history of detection\n",
    "threshold=0.7#To only give result if it is above threshold\n",
    "\n",
    "#Accessing video through webcm using OpenCV\n",
    "#We loop thorugh all frames in camera to create video\n",
    "cap=cv2.VideoCapture(0)#To acccess our webcam \n",
    "#here 0 represents device\n",
    "\n",
    "#Accessing the holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():#It checks wheather we are accessing or not\n",
    "        #Read feed\n",
    "        ret,frame=cap.read()#It reads our frames\n",
    "        \n",
    "        #Make detections\n",
    "        image,results=mediapipe_detection(frame,holistic)\n",
    "        \n",
    "        #Drwing Landmarks\n",
    "        draw_styled_landmarks(image,results)\n",
    "        \n",
    "        #Prediction Logic\n",
    "        keypoints=extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence=sequence[-30:]#Grabs last 30 frames\n",
    "        \n",
    "        #Run prediction if only 30 sequences collected\n",
    "        if len(sequence)==30:\n",
    "            res=model.predict(np.expand_dims(sequence,axis=0))[0]\n",
    "            \n",
    "            \n",
    "        #Rendering logic to show prediction in the opencv feed\n",
    "        #Checking wheather our result is above threshold\n",
    "        if res[np.argmax(res)].any()>threshold:\n",
    "            #Checking because we want to check next sequence\n",
    "            if len(sentence)>0:\n",
    "                #Checking if current action is not equal to last sentence\n",
    "                if actions[np.argmax(res)]!=sentence[-1]:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            else:\n",
    "                sentence.append(actions[np.argmax(res)])\n",
    "                \n",
    "        if len(sentence)>5:\n",
    "            #if sentence is greater than 5 grabbing last 5 value\n",
    "            sentence=sentence[-5:]\n",
    "            \n",
    "        \n",
    "        #Rendering to show the predection\n",
    "        cv2.rectangle(image,(0,0),(640,40),(245,117,16),-1)\n",
    "        cv2.putText(image,' '.join(sentence),(3,30),cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                       1,(255,255,255),2,cv2.LINE_AA)\n",
    "        \n",
    "        #Probabilities\n",
    "        try:\n",
    "            image=prob_viz(res,actions,image,colors)\n",
    "        except TypeError:\n",
    "            pass\n",
    "        \n",
    "        #To showw to screen\n",
    "        #Rendering\n",
    "        cv2.imshow(\"OpenCv Feed\",image)\n",
    "\n",
    "        #To Exit o break the feed\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break #it waits and if we press q breaks the loop\n",
    "    cap.release()#It releases the webcam\n",
    "    cv2.destroyAllWindows()#Destroy the cv window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To destroy open cv in middle\n",
    "cap.release()#It releases the webcam\n",
    "cv2.destroyAllWindows()#Destroy the cv window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0410fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
